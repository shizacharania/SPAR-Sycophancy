{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJYnHeX/jkELqvRj4Vid/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizacharania/Sycophancy-Research-SPAR/blob/main/shiza_repE_reading_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "wxtaZ6XhCwYC",
        "outputId": "7316ef9b-0af5-4de2-e8ed-8b5fc0ed96c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprerequisites for step one and two of implementing reading vector:\\n- need to have pairs of size 5 to 128\\n- determine the hidden state values with respect to the chosen LAT token position (which is [-2] for us) + probably pick a layer we want\\n- compute differences between hidden states within each pair, which is already done\\n- we normalize the difference -> should be [128, 4096], which 128 being the sample pairs and 4096 being the activation vector size\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "prerequisites for step one and two of implementing reading vector:\n",
        "- need to have pairs of size 5 to 128\n",
        "- determine the hidden state values with respect to the chosen LAT token position (which is [-2] for us) + probably pick a layer we want\n",
        "- compute differences between hidden states within each pair, which is already done\n",
        "- we normalize the difference -> should be [128, 4096], which 128 being the sample pairs and 4096 being the activation vector size\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "qsjw8ueUF684"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = np.random.rand(100,4096)\n",
        "data_tensor = torch.tensor(data_np)\n",
        "print(data_tensor)\n",
        "print(data_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9WJy6wSIyUw",
        "outputId": "629b6051-abff-47c7-fcee-ced313650db5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9994, 0.3158, 0.2256,  ..., 0.4628, 0.8987, 0.3470],\n",
            "        [0.6622, 0.0855, 0.3500,  ..., 0.8663, 0.7468, 0.3844],\n",
            "        [0.1995, 0.4673, 0.2130,  ..., 0.9818, 0.5591, 0.8015],\n",
            "        ...,\n",
            "        [0.8084, 0.1314, 0.8326,  ..., 0.4499, 0.3309, 0.1772],\n",
            "        [0.6366, 0.6613, 0.7788,  ..., 0.9422, 0.9872, 0.2158],\n",
            "        [0.9850, 0.3189, 0.7863,  ..., 0.2986, 0.0094, 0.8691]],\n",
            "       dtype=torch.float64)\n",
            "torch.Size([100, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize\n",
        "mean = torch.mean(data_tensor)\n",
        "std = torch.std(data_tensor)\n",
        "data_normalized = (data_tensor-mean)/std"
      ],
      "metadata": {
        "id": "VSc8EX-ldYPr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pca = torch.pca_lowrank(data_normalized) # no need to center because its normalized\n",
        "print(len(data_pca))\n",
        "print(data_pca[0].shape) # U\n",
        "print(data_pca[1].shape) # S\n",
        "# S**2/(samplesize-1) contains eigenvectors of ATA/(samplesize-1) which is the covariance of A when center=True is provided\n",
        "print(data_pca[2].shape) # V columns represent the principal directions\n",
        "\n",
        "first_pca = data_pca[2][:, :1] # gets the first principal component\n",
        "print(first_pca, first_pca.shape)\n",
        "first_pca = first_pca.squeeze()\n",
        "print(first_pca, first_pca.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0zyLUt9J7V_",
        "outputId": "a1e880e5-0443-434c-bc39-af6c007a6f5d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "torch.Size([100, 6])\n",
            "torch.Size([6])\n",
            "torch.Size([4096, 6])\n",
            "tensor([[ 0.0132],\n",
            "        [-0.0031],\n",
            "        [-0.0214],\n",
            "        ...,\n",
            "        [ 0.0129],\n",
            "        [ 0.0213],\n",
            "        [ 0.0151]], dtype=torch.float64) torch.Size([4096, 1])\n",
            "tensor([ 0.0132, -0.0031, -0.0214,  ...,  0.0129,  0.0213,  0.0151],\n",
            "       dtype=torch.float64) torch.Size([4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then you apply the first PCA on the same stimuli set S to obtain scores because\n",
        "# \"in practice, the 'reading vector' v is also multiplied by a 'sign' component\"\n",
        "scores = torch.tensor(np.dot(np.array(data_normalized), np.array(first_pca)))\n",
        "# each score should be one-dimensional, so dot product btwn normalized and first pca is needed\n",
        "big_dim_scores = data_normalized*first_pca\n",
        "print(scores.shape)\n",
        "print(big_dim_scores)\n",
        "print()\n",
        "# each stimulus has one score, which is why the shape of scores is torch.Size([100])\n",
        "updated_vectors = first_pca*scores[0] # one stimulus (in the loop, change the index based on the iteration you are on)\n",
        "print(updated_vectors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD5I_YDYMgvF",
        "outputId": "f39283d7-0f77-45ae-a6f2-8ce2e4e925d7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "tensor([[ 0.0229,  0.0020,  0.0203,  ..., -0.0017,  0.0294, -0.0080],\n",
            "        [ 0.0074,  0.0045,  0.0111,  ...,  0.0163,  0.0182, -0.0060],\n",
            "        [-0.0138,  0.0004,  0.0212,  ...,  0.0215,  0.0044,  0.0157],\n",
            "        ...,\n",
            "        [ 0.0141,  0.0040, -0.0246,  ..., -0.0022, -0.0124, -0.0168],\n",
            "        [ 0.0063, -0.0017, -0.0207,  ...,  0.0197,  0.0359, -0.0148],\n",
            "        [ 0.0222,  0.0020, -0.0212,  ..., -0.0090, -0.0361,  0.0192]],\n",
            "       dtype=torch.float64)\n",
            "\n",
            "torch.Size([4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "zlMKC0hqOeC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the hidden states at the token position [-2]\n",
        "# these are normalized uding the parameters derived using the construction of the PCA model in the training phase (mean and std variables from before)\n",
        "# calculate the dot product between the normalized hidden states and out reading vector, which yields a set of scores\n",
        "\n",
        "data_inference = np.random.rand(10,4096)\n",
        "data_inference_tensor = torch.tensor(data_np)\n",
        "data_inference_normalized = (data_inference_tensor-mean)/std\n",
        "data_inference_pca = torch.pca_lowrank(data_inference_normalized) # no need to center because its normalized\n",
        "\n",
        "first_pca_inference = data_inference_pca[2][:, :1] # gets the first principal component\n",
        "first_pca_inference = first_pca_inference.squeeze()\n",
        "print(first_pca_inference, first_pca_inference.shape)\n",
        "\n",
        "scores = data_inference_normalized*first_pca_inference\n",
        "print(scores, scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzDOt0umOdO-",
        "outputId": "a6f5d8b0-e69d-4cdf-cfc3-ec0df089bbb2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0080, -0.0195, -0.0016,  ...,  0.0171, -0.0279,  0.0091],\n",
            "       dtype=torch.float64) torch.Size([4096])\n",
            "tensor([[ 0.0138,  0.0125,  0.0015,  ..., -0.0022, -0.0386, -0.0048],\n",
            "        [ 0.0045,  0.0280,  0.0008,  ...,  0.0217, -0.0239, -0.0036],\n",
            "        [-0.0083,  0.0022,  0.0016,  ...,  0.0286, -0.0057,  0.0095],\n",
            "        ...,\n",
            "        [ 0.0086,  0.0249, -0.0019,  ..., -0.0030,  0.0163, -0.0101],\n",
            "        [ 0.0038, -0.0109, -0.0016,  ...,  0.0262, -0.0471, -0.0089],\n",
            "        [ 0.0134,  0.0122, -0.0016,  ..., -0.0119,  0.0474,  0.0116]],\n",
            "       dtype=torch.float64) torch.Size([100, 4096])\n"
          ]
        }
      ]
    }
  ]
}